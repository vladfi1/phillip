runner.py - config file that writes out a json
	name and tag no need to pass shit (uniquely identify)
	model = Actor Critic (only one that works) (he is going to fix rac.py)
	DQN -> Q values easier to interpret -> More like supervised learning, decreasing loss
		Regress past q values to future values
		Sarsa uses actual q values while DQN uses max q values -> learn on policy q value
		maxq -> very unstable (most overoptimistic approximation) -> off policy as trying to learn optimal policy
	Basically rdqn and rac should both be tested, but rac is what's going to be fixed for now

	natural (natural gradient) = false (might want to try natural for RNN (add stability)) (kinda weird tho)
	tdN - How far in future do you want to cutoff Monte Carlo estimates (Where we truncate the sum) (Don't even use that) (Not used anymore as he goes to the end of the episode)
	reward_half - discount factor (2 is high, much less greedy, but much high variance)
	sweeps, batches, batch size, batch steps (recurrent net uses a lot of memory, might have to decrease batch size)

	temperature - q values divide by temperature and take softmax (high temperature,high entropy) (temperature same scale as variance of q values) (should be)
	epsilon puts lower bound on the entropy so put it in actor critic because whatever
	entropy - entropy too high (3.7) lower entropy scale, too low (0.02) raise entropy scale (if entropy crashes look at this)

	learning rate - order of magnitude less for recurrent (temporal stability) (might want to fiddle)
	Optimizer - Adam, look into that (False means not included in unique name)
	Activation Function - ELU
	act_every - maximize act_every (Action delay)
	delay - In frames is delay*act_every (12-15 frames is human)
	enemies - string - read from a file, enemies files specify a json which indicates what agents to use as enemies

launcher.py - 
	path - path to experiment (save/experiment_name)
	dry_run - dont actually run but print out the commands
	init - initialize model (first time running experiment)

	launcher - Run twice -> 1st time - Don't specify trainer argument (default - Run trainer, starts a slurm job)
		slurm has a queue of jobs with priorities etc, launcher put in queue, and then it'll run
		Command line output includes job id, "scontrol show job [jobid]", check if job has started yet (if not started, it'll say pending)
			if running, will say what machine its running on (nodes 1 - 54)
		2nd time --trainer [node number]
		agents - override default of 120 agents - (node-free, if more cores/memory free, more agents)

		qos - priority - (tell how many cpus are available with node-free, priority hierarchy -> tenenbaum, normal, everything)
			sq command -> CG - Garbage Collected PD - Pending R - Running
			k20 - better, half as much memory, much slower, stick with titanx
			gpu -> titanx if not enough titanx then do 1
			qos -> Trainer tenenbam (if not normal), Agent normal (if not everything) (if you comment it out its normal)

Train 

	On-policy - data you're training on is from the same policy (More restrictive)
	Original DQN is off policy

train.py - 
	Default -> anything in _options set as members of the class
	**kwargs -> Takes all keyword arguments and puts it in a dict kwargs -> Default class sets proper class members
	init -> initialize model, if not true then loads parameters
	min_collect -> how many experiences to gather before doing more training (default - 1) (might want to increase if dont have enough agents)
	load -> Gets folder with params generated by runner and snapshots

RL.py - Main RL class
	Mode(Enum) If mode set to 0 train (trainer), if 1 play (agent)
	path -> Where to save/Where to pull from
	GameEmbedding -> Embeds the game's features into vectors (states)

	Does a lot of slicing...Basically doing its job, dont need to change shit


A3C - Synchronous (gradient updates done synchronously) (might be better than the pure A3C)
Rac - GRU just a variant on LSTM (using own GRU implementation (changed minor variable scoping))
	Tune recurrent net - Change actor layers etc

phillip --help to get help on all the stuff

rac.py
	actor_layers -> 128x128
	dynamic -> Makes dynamic an option, 
	Dynamic RNN, makes RNN faster -> Doesnt work with natural gradients, but natural gradients should be good for RNN (but not necessary)

	history -> recent history at that point in time
	Feeding in state and action, because previous action matters

	GRUCell, custom because of stupid reasons (LSTM cell implementation) (in tf_lib)
	All get wrapped into a MultiRNN cell
	makeaffinelayer -> fclayer (basically)

	Want to increase actor_gain objective function (Stays around 0, advantages are always mean 0)

Look into RNN RL resources
Check out Open AI agent
Paper Vlad cites -> Gradient Clipping
Log other shit

Play against ingame level 9 cpu, and look at the logs
