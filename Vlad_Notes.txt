runner.py - config file that writes out a json
	name and tag no need to pass shit (uniquely identify)
	model = Actor Critic (only one that works) (he is going to fix rac.py)
	DQN -> Q values easier to interpret -> More like supervised learning, decreasing loss
		Regress past q values to future values
		Sarsa uses actual q values while DQN uses max q values -> learn on policy q value
		maxq -> very unstable (most overoptimistic approximation) -> off policy as trying to learn optimal policy
	Basically rdqn and rac should both be tested, but rac is what's going to be fixed for now

	natural (natural gradient) = false (might want to try natural for RNN (add stability)) (kinda weird tho)
	tdN - How far in future do you want to cutoff Monte Carlo estimates (Where we truncate the sum) (Don't even use that) (Not used anymore as he goes to the end of the episode)
	reward_half - discount factor (2 is high, much less greedy, but much high variance)
	sweeps, batches, batch size, batch steps (recurrent net uses a lot of memory, might have to decrease batch size)

	temperature - q values divide by temperature and take softmax (high temperature,high entropy) (temperature same scale as variance of q values) (should be)
	epsilon puts lower bound on the entropy so put it in actor critic because whatever
	entropy - entropy too high (3.7) lower entropy scale, too low (0.02) raise entropy scale (if entropy crashes look at this)

	learning rate - order of magnitude less for recurrent (temporal stability) (might want to fiddle)
	Optimizer - Adam, look into that (False means not included in unique name)
	Activation Function - ELU
	act_every - maximize act_every (Action delay)
	delay - In frames is delay*act_every (12-15 frames is human)
	enemies - string - read from a file, enemies files specify a json which indicates what agents to use as enemies

launcher.py - 
	path - path to experiment (save/experiment_name)
	dry_run - dont actually run but print out the commands
	init - initialize model (first time running experiment)

	launcher - Run twice -> 1st time - Don't specify trainer argument (default - Run trainer, starts a slurm job)
		slurm has a queue of jobs with priorities etc, launcher put in queue, and then it'll run
		Command line output includes job id, "scontrol show job [jobid]", check if job has started yet (if not started, it'll say pending)
			if running, will say what machine its running on (nodes 1 - 54)
		2nd time --trainer [node number]
		agents - override default of 120 agents - (node-free, if more cores/memory free, more agents)

		qos - priority - (tell how many cpus are available with node-free, priority hierarchy -> tenenbaum, normal, everything)
			sq command -> CG - Garbage Collected PD - Pending R - Running
			k20 - better, half as much memory, much slower, stick with titanx
			gpu -> titanx if not enough titanx then do 1
			qos -> Trainer tenenbam (if not normal), Agent normal (if not everything) (if you comment it out its normal)

Train 

	On-policy - data you're training on is from the same policy (More restrictive)
	Original DQN is off policy

train.py - 
	Default -> anything in _options set as members of the class
	**kwargs -> Takes all keyword arguments and puts it in a dict kwargs -> Default class sets proper class members
	init -> initialize model, if not true then loads parameters
	min_collect -> how many experiences to gather before doing more training (default - 1) (might want to increase if dont have enough agents)
	load -> Gets folder with params generated by runner and snapshots

RL.py - Main RL class
	Mode(Enum) If mode set to 0 train (trainer), if 1 play (agent)
	path -> Where to save/Where to pull from
	GameEmbedding -> Embeds the game's features into vectors (states)

	Does a lot of slicing...Basically doing its job, dont need to change shit


A3C - Synchronous (gradient updates done synchronously) (might be better than the pure A3C)
Rac - GRU just a variant on LSTM (using own GRU implementation (changed minor variable scoping))
	Tune recurrent net - Change actor layers etc

phillip --help to get help on all the stuff

rac.py
	actor_layers -> 128x128
	dynamic -> Makes dynamic an option, 
	Dynamic RNN, makes RNN faster -> Doesnt work with natural gradients, but natural gradients should be good for RNN (but not necessary)

	history -> recent history at that point in time
	Feeding in state and action, because previous action matters

	GRUCell, custom because of stupid reasons (LSTM cell implementation) (in tf_lib)
	All get wrapped into a MultiRNN cell
	makeaffinelayer -> fclayer (basically)

	Want to increase actor_gain objective function (Stays around 0, advantages are always mean 0)

Look into RNN RL resources
Check out Open AI agent
Paper Vlad cites -> Gradient Clipping
Log other shit

Play against ingame level 9 cpu, and look at the logs






Try running regular actor critic, make recurrent empty, to compare to a baseline
Weight initialization - Random normal, make it have norm 1 (scale weights) (scaled_weight_initializer implements this (you'd have to return weight, scale, and bias))
	https://arxiv.org/pdf/1602.07868.pdf
GRU Cell doesn't have to use getvariable, has its own activations, worth looking at GRU implementation (tanh - activation for GRU cell)

Entropy -> e^(entropy) actions (Entropy drops but doesnt drop a lot as it learns) (decrease the entropy scale to have it drop further)
Learning Rate -> Fiddle with that
reward_halflife -> (drop) 2 (think about how much you care about x seconds in the future) (delay - quarter second, so having reward_halflife 4 or 5 is ridiculous)

v_loss, v_uev (Unexplained Variance??? Think of R^2) - critic

To understand actor loss, look at math for REINFORCE (why that's a policy gradient) (It's not actually a loss, entropy term kinda operates like a loss, actor_gain doesnt)
	Look at description of entropy in paper

Yes, don't train against delay 0 agents, master level 9 under human-like delay, than train against yourself, then have new char agents train against each other etc (do whats in the paper)

DQN has a loss, actor critic loss isnt really a loss that is minimized (so adding auxillary task penalty to loss kinda makes sense for DQN, not really for A3C)
Auxillary control problem - Learning to solve seperate fake tasks, won't inherently be able to resolve action delay problem (what agent actually learns through auxillary tasks is a mystery)

Learning transition function is supervised learning problem (prediction loss), pixel control (unsupervised RL problem) (RL task)

We see the world by predicting where things will be, because can't react to its perfectly (happens subconsciously)
	Agent knows its x delayed, hence asks model to predict x states ahead, and so is fed back the present state

If model is sufficiently trained, whether agent is recurrent or not is insignificant
	Hence, approaches are orthogonal (I do think though that if the agent learns to interpret the fed back state, rather than just taking it as fact, the fact that its recurrent could help)

However, these are premature thoughts, as a model-based approach to resolving the delay problem is much harder, so we should focus on the RNN
